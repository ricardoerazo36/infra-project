import os
import json
import glob
import time
import collections
import pandas as pd
from datetime import datetime

CLEAN_DIR = "/app/data/clean"
OUT_DIR = "/app/data/analysis"

os.makedirs(OUT_DIR, exist_ok=True)

# Palabras clave por categor√≠a 
topics = {
    "economia": ["econom√≠a", "econ√≥mico", "colcap", "bvc", "inflaci√≥n", "d√≥lar", "tasas"],
    "seguridad": ["sicario", "asesinato", "homicidio", "violencia", "incidente", "capturado", "paro armado", "eln", "ataque", "explosi√≥n"],
    "politica": ["gobierno", "ministro", "presidente", "congreso", "alcalde", "elecciones", "pol√≠tica"],
    "salud": ["salud", "hospital", "covid", "enfermedad", "cl√≠nica", "medicina"],
}

def analyze_news():
    """Analiza noticias y cuenta por categor√≠a"""
    print(f"[{datetime.now()}] Iniciando an√°lisis de noticias...")
    
    daily_counts = {}
    total_analyzed = 0

    # Buscar todos los archivos JSON en clean
    clean_files = glob.glob(os.path.join(CLEAN_DIR, "*.json"))
    
    print(f"üìÑ Archivos a analizar: {len(clean_files)}")

    for path in clean_files:
        try:
            with open(path, "r", encoding="utf-8") as f:
                doc = json.load(f)

            text = (doc.get("title", "") + " " + doc.get("text", "")).lower()

            # Fecha de la noticia
            date = doc.get("publish_date")
            if not date:
                date = datetime.utcnow().isoformat()

            day = date.split("T")[0]

            if day not in daily_counts:
                daily_counts[day] = collections.Counter()

            # Contar por cada tema
            for topic, words in topics.items():
                if any(word in text for word in words):
                    daily_counts[day][topic] += 1
            
            total_analyzed += 1
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error procesando {path}: {e}")

    # Convertir Counter a dict normal para JSON
    daily_counts_serializable = {}
    for day, counter in daily_counts.items():
        daily_counts_serializable[day] = dict(counter)

    # Guardar resultado
    output_file = os.path.join(OUT_DIR, "daily_counts.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(daily_counts_serializable, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ An√°lisis completado:")
    print(f"   - Archivos analizados: {total_analyzed}")
    print(f"   - D√≠as con datos: {len(daily_counts)}")
    print(f"   - Archivo generado: {output_file}")
    
    # Mostrar resumen
    if daily_counts:
        print(f"\nüìä Resumen por d√≠a:")
        for day in sorted(daily_counts.keys())[-3:]:  # √öltimos 3 d√≠as
            counts = daily_counts[day]
            print(f"   {day}: {dict(counts)}")
    
    return len(daily_counts)

def main():
    """Loop principal del analizador"""
    interval = int(os.getenv("SLEEP_INTERVAL", 1800))
    
    print("üöÄ Iniciando analizador de noticias...")
    print(f"‚è±Ô∏è  Intervalo: {interval} segundos ({interval/60:.1f} minutos)\n")
    
    while True:
        try:
            days_analyzed = analyze_news()
            
            if days_analyzed == 0:
                print("‚ö†Ô∏è  No hay datos para analizar a√∫n")
            
        except Exception as e:
            print(f"‚ùå Error en ciclo principal: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"\nüí§ Esperando {interval} segundos...\n")
        time.sleep(interval)

if __name__ == "__main__":
    main()